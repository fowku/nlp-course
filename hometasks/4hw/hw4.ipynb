{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 4: topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе мы попытаемся обучить LDA-модель topic-моделингу на двух принципиально различных корпусах. \n",
    "\n",
    "В первой части вы познакомитесь с новыми возможностями библиотеки gensim, а также с возможностями парсинга в языке Python. Во второй части вам предстоит самостоятельно обучить LDA-модель и оценить качество её работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: topic modeling уровня /b/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краеугольным камнем в машинном обучений в целом, и в NLP в частности, является выбор датасетов. Доселе мы использовали только стандартные, многократно обкатанные датасеты, но сегодня попробуем собрать свой. Практика работы с сырыми, необработанными данными весьма полезно! Заодно изучим возможности парсеров в Питоне.\n",
    "\n",
    "Давайте напишем парсер, собирающий информацию о сообщения с русскомязычного анононимного форума (имиджборды) \"Двач\" (\"Сосач\", \"Хиккач\", если вам угодно). Двач, как и всякая имиджборда разделён на разделы (доски, борды), посвященные различным тематикам -- аниме, видеоигры, литература, религия... Каждая доска состоит из тем (тредов, топиков), которые создаются анонимными (при их желании) пользователями. Каждый тред посвящен обсуждению какого-то конкретного вопроса.\n",
    "\n",
    "У некоторых разделов есть раздел архив, располагается он по адресу https://2ch.hk/(название раздела)/arch/, например для раздела музыка -- https://2ch.hk/mu/arch/. Если у вас есть минимальные навыки в языке html, а также если вы изучили документацию встроенного класса HTMLParser, то вам будет несложно написать два парсера.\n",
    "\n",
    "Первый парсер (ArchiveParser) парсит HTML-страницу архива доски, вытягивает из неё ссылки на заархивированные треды, и скармливает их второму парсеру.\n",
    "\n",
    "Второй парсер (ThreadParser) парсит HTML-страницу заархивированного треда, вытягивает из неё сообщения, складывает их вместе и собирает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def get_value_by_key(attrs, key):\n",
    "    for (k, v) in attrs:\n",
    "        if(k == key):\n",
    "            return v;\n",
    "    return None\n",
    "\n",
    "class ArchiveParser(HTMLParser):\n",
    "    flag = False\n",
    "    threads = []\n",
    "    limit = 200\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(self.limit > 0):\n",
    "            if(tag == 'div'):\n",
    "                cl = get_value_by_key(attrs, 'class')\n",
    "                if (cl == 'box-data'):\n",
    "                    self.flag = True;\n",
    "            if(self.flag == True and tag == 'a'):\n",
    "                href = get_value_by_key(attrs, 'href')\n",
    "                if(len(href)>20):\n",
    "                    print(href)\n",
    "                    print(self.limit)\n",
    "                    thread = parse_thread('https://2ch.hk' + href)\n",
    "                    if(len(thread) > 10):\n",
    "                        self.threads.append(thread)\n",
    "                        self.limit = self.limit - 1\n",
    "                    thread = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'div'):\n",
    "            self.flag = False;\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        1+1\n",
    "        \n",
    "    def get_threads(self):\n",
    "        return self.threads\n",
    "    \n",
    "    def clean(self):\n",
    "        self.threads = []\n",
    "        \n",
    "parser = ArchiveParser()\n",
    "\n",
    "def parse_archive(board = '/b/', page_number = 0):\n",
    "    lines = []\n",
    "    link = 'https://2ch.hk' + board + 'arch/' + str(page_number) +'.html'\n",
    "    print(link)\n",
    "    parser.limit = 100\n",
    "    url = urllib.request.urlopen(link)\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8'))\n",
    "    for line in lines:\n",
    "        parser.feed(line)\n",
    "    res = parser.get_threads()\n",
    "    parser.clean()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadParser(HTMLParser):\n",
    "    flag = False\n",
    "    message = []\n",
    "    messages = []\n",
    "            \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = True;\n",
    "            self.message = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = False\n",
    "            if(self.message != []):\n",
    "                self.messages.append(self.message)\n",
    "            self.message = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if(self.flag):\n",
    "            self.message.extend(simple_preprocess(data))\n",
    "            \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def clear_messages(self):\n",
    "        flag = False\n",
    "        self.message = []\n",
    "        self.messages = []\n",
    "\n",
    "t_parser = ThreadParser()\n",
    "\n",
    "def parse_thread (link):\n",
    "    url = urllib.request.urlopen(link)\n",
    "    lines = []\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8', errors='ignore'))\n",
    "    for line in lines:\n",
    "        t_parser.feed(line)\n",
    "    res = t_parser.get_messages()\n",
    "    t_parser.clear_messages()\n",
    "    #print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весьма много кода, верно? Если не потерялись, могли заметить функцию parse_archive, которая парсит страницу архива по доске и номеру страницы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "Давайте применим её к каким-нибудь доскам. Выберите две доски двача, имеющие архив и скачайте архивы функцией parse_archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boards = ['] #TODO: напишите название досок в формате /'доска'/, например /mu/ для Музыки\n",
    "threads_by_topic = [parse_archive(board=board) for board in boards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данны на тренировочые и тестовые. Пусть каждый десятый тред попадает в тест-сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "test = []\n",
    "\n",
    "it = 0\n",
    "for topic in threads_by_topic:\n",
    "    for thread in topic:\n",
    "        full = []\n",
    "        for post in thread:\n",
    "            full.extend(post)\n",
    "        it = it + 1\n",
    "        if(it % 10 == 0):\n",
    "            test.append(full)\n",
    "        else:\n",
    "            data.append(full)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "В русском языке есть множество слов (частицы, междометия, всё что вы хотите), которые никак не отображают смысл слов и являются вспомогательными. Чтобы ваша модель работала лучше -- добавьте стоп-слова в список RUSSIAN_STOP_WORDS или в строку st_str. Эти слова отфильтруются из датасета перед тем, как модель начнет обучаться на датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "RUSSIAN_STOP_WORDS = ['не', 'это', 'что','чем','это','как','https','нет','op','он','же','так','но','да','нет','или','и', 'на', \"то\", \"бы\", \"все\", \"ты\", \"если\", \"по\", \"за\", \"там\", \"ну\", \"уже\", \"от\", \"есть\",\"был\", \"даже\", \"было\", \"www\", \"com\", \"youtube\", \"из\", \"будет\", \"mp\", \"они\", \"только\", \"его\", \"она\", \"вот\", 'просто', 'watch', 'кто', 'для', 'когда', 'тут', 'мне', 'где', 'мы', 'какой', 'может', 'меня', 'до', 'про', 'http', 'раз', 'почему', 'тебя', 'ещё', 'их', 'сейчас', 'тоже', 'во', 'чтобы', 'этого','без', 'него','вы','такой', 'можно', 'надо', 'нахуй', 'ли', 'потом', 'тред', 'больше', 'лучше', 'хуй', 'сам', 'после', 'со', 'лол', 'быть', 'нужно', 'этом', 'блять', 'бля', 'того', 'ничего', 'потому', 'нибудь', 'этот', 'под', 'через', 'ни', 'себе', 'ему', 'при', 'какие', 'пиздец', 'теперь', 'хоть', 'говно', 'тогда', 'блядь', 'кстати', 'че', 'себя', 'конечно', 'типа', 'много', 'том', 'нихуя', 'куда', 'всегда', 'нас', 'тот', 'ведь', 'эти', 'них', 'сука', 'пока', 'более', 'чего', 'html', 'были', 'всех', 'была', 'например', 'тем', 'ru', 'зачем', 'либо', 'вроде', 'всего', 'вопрос', 'php', 'против', 'здесь', 'ее', 'значит', 'совсем', 'сколько', 'им', 'org', 'именно', 'эту',]\n",
    "st_str = \"которых которые твой которой которого сих ком свой твоя этими слишком нами всему будь саму чаще ваше сами наш затем еще самих наши ту каждое мочь весь этим наша своих оба который зато те этих вся ваш такая теми ею которая нередко каждая также чему собой самими нем вами ими откуда такие тому та очень сама нему алло оно этому кому тобой таки твоё каждые твои мой нею самим ваши ваша кем мои однако сразу свое ними всё неё тех хотя всем тобою тебе одной другие этао само эта буду самой моё своей такое всею будут своего кого свои мог нам особенно её самому наше кроме вообще вон мною никто это\"\n",
    "RUSSIAN_STOP_WORDS.extend(st_str.split(' '))\n",
    "\n",
    "data = [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in data]\n",
    "id2word = corpora.Dictionary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь и на его основе преобразуем слова в их id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим LDA-модель, используя библиотеку gensim. Зададим число тем равно числу скачанных досок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, num_topics=len(threads_by_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим топ-10 самых используемых в каждой теме слов.\n",
    "\n",
    "$\\textbf{Задание.}$\n",
    "Оцените насколько хорошо модель разделила темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(threads_by_topic)):\n",
    "    print([id2word[id[0]] for id in model.get_topic_terms(topicid = i, topn = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прогоним тестовые треды на модели. Тестовый датасет разделен на n равных частей по 20 тредов, i-ая соответствует i-й доске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_corpus = [id2word.doc2bow(text) for text in [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in test]]\n",
    "\n",
    "vector = [model[unseen_doc] for unseen_doc in other_corpus]\n",
    "print(vector[0]) #вероятности принадлежности 0-го тестового треда в ту или иную тему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for res in vector:\n",
    "    max_it = 0\n",
    "    if(len(res) > 0):\n",
    "        for it in range(1, len(res)):\n",
    "            if(res[max_it][1] < res[it][1]):\n",
    "                max_it = it\n",
    "        print(\"Text #\" + str(i) + \", topic #\" + str(max_it) + str(\", prob = \" + str(res[max_it][1])))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "\n",
    "Оцените результаты работы модели на тест сете. Если модель разделили данные плохо -- объясните, почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. А теперь нормальный датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте воспользуемся более стандартным датасетом библиотеки sklreatn -- 20newsgroups, посвященную статьям на различные темы. Выберем 6 -- Атеизм, яблочное железо, автомобили, хоккей, космос, христианство, ближний восток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'rec.autos',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories = categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание}$\n",
    "\n",
    "Найдите библиотечный или опишите свой список ENGSLISH_STOP_WORDS, убирающий не несущие никакого смысла английские слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "ENGLISH_STOP_WORDS = [] #TODO\n",
    "\n",
    "print('the' in ENGLISH_STOP_WORDS) \n",
    "\n",
    "data = [list(filter(lambda word: not word in ENGLISH_STOP_WORDS, simple_preprocess(piece))) for piece in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 1.}$\n",
    "\n",
    "Для списка data создайте словарь id2word. Получите преобразованный TermDocumentFrequency список corpust и обучите на нем LDA модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LsiModel\n",
    "\n",
    "print(data[0])\n",
    "id2word = #TODO\n",
    "\n",
    "# Create Corpus\n",
    "texts = data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = #TODO\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "model = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выведем получившийся список тем:\n",
    "for i in range(len(categories)):\n",
    "    print([id2word[id[0]] for id in model.get_topic_terms(topicid = i, topn = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 2.}$\n",
    "\n",
    "В соответствии с тренировочными, обработайте тестовые данные.\n",
    "\n",
    "Напишите функцию, которая с помощью модели возвращает наиболее вероятный id темы. С помощью F-меры оцените правильность работы модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories = categories)\n",
    "\n",
    "#TODO: YOUD CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
