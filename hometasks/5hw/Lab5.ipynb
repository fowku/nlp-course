{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа #5: Part-of-Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предстоит построить модель, решающую задачу классификации слов в предложении на части речи. В основе этой модели должна лежать скрытая марковская модель (HMM). О том, что это такое и как с этим работать рассказывалось на лекции.\n",
    "\n",
    "Каждая из возможных частей речи соовтетствует некоторому скрытому HMM-модели, слова играют роль наблюдений, а матрица вероятностей переходов определяется подсчётом би-грамм и уни-грамм в тренировочном датасете. Допустим мы имеем некоторую последовательность наблюдений (слов) $W = {w_1..w_N}$ и некоторую настроенную HMM-модель. Тогда, применив ним алгоритм Витерби $\\href{https://neerc.ifmo.ru/wiki/index.php?title=Алгоритм_Витерби}{алгоритм Витерби}$, мы можем получить наиболее вероятную последовательность скрытых состояний $Q = {q_1..q_N}$, каждое из которых соответствует некоторой части речи. Таким образом, для каждого слова мы получим его часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOTd23QOHxzd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import random\n",
    "import string\n",
    "from collections import namedtuple, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных и feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета используем упрощённую версию $\\href{https://www.kaggle.com/nltkdata/brown-corpus}{Brown Corpus}$. Этот датасет содержит в себе ~57k последовательностей слов (предложений) на английском языке. Для каждого слово в каждой последовательности указана его часть речи. Суммарно, датасет включает в себя слова 11 различных частей речи. \n",
    "\n",
    "Как уже было отмечено, вероятностная матрица переходов определяется через подсчёт случаев (т.е. вероятность $P({w_i}|{q_j}) = \\dfrac{Count(q_j, w_i)}{Count(q_j)}$). Если модель встречает слово $w$, которого не было в тренировочном наборе данных, то $P(w|q)$ будет равна нулю для любого скрытого состояния q. Соответственно, вероятность любой последовательности слов, содежащей w, будет равна нулю, и модель не сможет оценить такую последовательность. Аналогичная ситуация будет в случае со словами с опечатками, с очень редкими словами и прочими.\n",
    "\n",
    "Мы хотим, чтобы наша модель работала на любых тестовых данных, в том числе содержащих неизвестные модели слова. Для этого давайте вычленять из слов некоторые признаки (features, фичи) и использовать их вектора для расчёта вероятностей вместо слов. Например, возможный набор фичей может выглядеть так:\n",
    "(длина слова если она не более $5$ иначе $5$, является ли слово первым в предложении, содержит ли слово цифры, начинается ли предыдущее слово с большой буквы). Для предложенного набора фич, последовательность слов ['Kill', 'me', 'plea5e'] закодируется как $[[4, 1, 0], [2, 0, 0, 1], [6, 0, 1, 0]]$. Несложно заметить, что область возможных значений векторов-признаков равна 5 * 2 * 2 * 2 = 40. Хорошей идеей будет попытаться выбрать вектора так, чтобы слова одной части речи с бОльшей вероятностью кодировались одинаковыми векторами-признаками.\n",
    "\n",
    "Чем меньше фич, чем меньше область возможных значений вектора признаков, тем больше слов кодируются одинаковыми векторами, тем хуже предсказания модели. Однако чем больше область воможных значений векторов-признаков, тем больше вероятность встретить в тестовом наборе данных слово, кодирование которого не встретилось в тренировочном датасете.\n",
    "\n",
    "$\\bf{Задание.}$ Придумайте какие-нибудь (потенциально полезные) признаки, и добавьте их в функцию $\\textit{extract_features}$, принимающую последовательность слов в качестве tuple. и возвращающую последовательность векторов-признаков. Область возможных значений векторов-фичей должна быть достаточно широкой, чтобы модель чему-то обучилась, но при этом достаточно маленькой, чтобы ни одно слово из тестового набора не закодировалось невстреченным доселе  вектором-признаком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6F2nTljHxzt"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('Kill', 'me', 'plea5e')\n[[4, 1, 0, 0, 0, 0, 1, 3, 0, 1, 108.0], [2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 105.0], [5, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 77.0]]\n[4, 1, 0, 0, 0, 0, 1, 3, 0, 1, 108.0] [2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 105.0] [5, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 77.0]\n"
    }
   ],
   "source": [
    "def extract_features(tupled_words):\n",
    "    \n",
    "    feat = []\n",
    "    n = len(tupled_words)\n",
    "    for i in range(n):\n",
    "        \n",
    "        #очередное слово\n",
    "        word = tupled_words[i]\n",
    "        \n",
    "        #вектор признаков для текущего слова\n",
    "        res = []\n",
    "        \n",
    "        #длина слова, если она меньше 5, иначе 5\n",
    "        res.append(5 if len(word) >= 5 else len(word))\n",
    "\n",
    "        #является ли первый символ заглавной буквой\n",
    "        res.append(int(word[0] <= 'Z' and word[0] >= 'A'))\n",
    "\n",
    "        # has ing\n",
    "        res.append(int(word[-3:] == 'ing'))\n",
    "\n",
    "        # has ed\n",
    "        res.append(int(word[-2:] == 'ed'))\n",
    "\n",
    "        # has -\n",
    "        res.append(int('-' in word))\n",
    "\n",
    "        # has –\n",
    "        res.append(int('–' in word))\n",
    "\n",
    "        # num of vowels\n",
    "        res.append(len([letter for letter in word.upper() if letter in 'AIEOUY']))\n",
    "\n",
    "        # num of consonants\n",
    "        res.append(len([letter for letter in word.upper() if letter in 'BCDFGHJKLMNPQRSTVWXZ']))\n",
    "\n",
    "        # ratio of letters dictionary to number of letters\n",
    "        res.append( len(set(word)) // len(word) )\n",
    "\n",
    "        if (i > 0):\n",
    "            # начинается ли предыдущее слово с заглавной буквы\n",
    "            res.append(int(tupled_words[i - 1][0].isupper()))\n",
    "\n",
    "            # previous is article\n",
    "            res.append(int(tupled_words[i - 1].lower() == 'a'))\n",
    "            res.append(int(tupled_words[i - 1].lower() == 'an'))\n",
    "            res.append(int(tupled_words[i - 1].lower() == 'the'))\n",
    "        else:\n",
    "            res.append(1) #first one\n",
    "\n",
    "        # mean of previous 2 letters of the word\n",
    "        res.append(np.mean([ord(letter) for letter in word[-2:]])) if (len(word) > 1) else res.append(0)\n",
    "                    \n",
    "        feat.append(res)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def hash_feature_list(coded_word):\n",
    "    \"\"\"вспомогательная функция, позволяющая преобразовать список \n",
    "    в хешируемый вид (строку)\"\"\"\n",
    "    return ' '.join(map(lambda x: str(x), coded_word))\n",
    "\n",
    "example = tuple(['Kill', 'me', 'plea5e'])\n",
    "print(example)\n",
    "print(extract_features(example))\n",
    "print(hash_feature_list(extract_features(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Dataset имплементирует чтение последовательностей слов (и их тегов) из файла, разделение на тренировочную и тестовую выборки и подсчёт некоторой статистики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4r21Vz0rHxzx"
   },
   "outputs": [],
   "source": [
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "\"\"\"Функции для парсинга предложений и тегов\"\"\"\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "def read_tags(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab c_vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([extract_features(sentences[k].words) for k in keys])\n",
    "        uncoded_word_sequences = tuple([tuple([word for word in sentences[k].words]) for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*uncoded_word_sequences))\n",
    "        codeset = frozenset([hash_feature_list(x) for x in chain(*word_sequences)])\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, codeset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "def __len__(self):\n",
    "        return len(self.sentences)\n",
    "def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D9t3nBLHxz2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Всего 57340 предложений.\nИз них 45872 -- в тренировочном наборе.\nА остальные 11468 -- в тестовом.\nВ тренировочном наборе данных встречается 50656 уникальных слов, они кодируются в 18650 различных векторов-признаков.\nВ тестовом наборе данных 25002 уникальных слов, каждое из которых соответствует одному из 10995 различных векторов.\nВ тестировочном датасете встречается 5401 уникальных слов, которые не встречались в тренировочном наборе, а также 1462 векторов-признаков, не встречающихся в трейне.\n"
    }
   ],
   "source": [
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream ustream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=239):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "                \n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "                \n",
    "        word_sequences = tuple([extract_features(sentences[k].words) for k in keys])\n",
    "        uncoded_word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        codeset = frozenset([hash_feature_list(x) for x in chain(*word_sequences)])\n",
    "        \n",
    "        # делим на трейн/тест\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        ustream = tuple(zip(chain(*uncoded_word_sequences), chain(*tag_sequences)))\n",
    "        \n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__, ustream.__iter__)\n",
    "def __len__(self):\n",
    "        return len(self.sentences)\n",
    "def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "print(\"Всего {} предложений.\".format(len(data.sentences)))\n",
    "print(\"Из них {} -- в тренировочном наборе.\".format(len(data.training_set.sentences)))\n",
    "print(\"А остальные {} -- в тестовом.\".format(len(data.testing_set.sentences)))\n",
    "print(\"В тренировочном наборе данных встречается {} уникальных слов, они кодируются в {} различных векторов-признаков.\"\n",
    "      .format(len(data.training_set.vocab), len(data.training_set.c_vocab)))\n",
    "print(\"В тестовом наборе данных {} уникальных слов, каждое из которых соответствует одному из {} различных векторов.\"\n",
    "      .format(len(data.testing_set.vocab), len(data.testing_set.c_vocab)))\n",
    "print(\"В тестировочном датасете встречается {} уникальных слов, которые не встречались в тренировочном наборе, а также {} векторов-признаков, не встречающихся в трейне.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab), len(data.testing_set.c_vocab - data.training_set.c_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nStream (feature_vector, tag) pairs:\n\n\t Mr. ([3, 1, 0, 0, 0, 0, 0, 2, 1, 1, 80.0], 'NOUN')\n\t Podger ([5, 1, 0, 0, 0, 0, 2, 4, 1, 1, 0, 0, 0, 107.5], 'NOUN')\n\t had ([3, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 98.5], 'VERB')\n\t thanked ([5, 0, 0, 1, 0, 0, 2, 5, 1, 0, 0, 0, 0, 100.5], 'VERB')\n\t him ([3, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 107.0], 'PRON')\n"
    }
   ],
   "source": [
    "print(\"\\nStream (feature_vector, tag) pairs:\\n\")\n",
    "i = 5\n",
    "for word, pair in zip(data.ustream(), data.stream()):\n",
    "    print(\"\\t\", word[0], pair)\n",
    "    i = i - 1\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подсчёт вероятностей и построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество встречающихся би- и уни-грамм тэгов (надеюсь вы не забыли что такое n-граммы, с тех пор как сдавали ДЗ #2). Это необходимо, чтобы определить матрицу переходов между скрытыми состояниями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_qVHkt1Hx3O"
   },
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "    return Counter(sequences)\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tag_unigrams = unigram_counts(tags)\n",
    "\n",
    "def bigram_counts(sequences):\n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "o = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]\n",
    "tag_bigrams = bigram_counts(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления условных вероятностей, а также стартового и конечного распределения состояний, нам необходимо посчитать для каждого лейбла (скрытого состояния): (А) количество векторов-признаков, помеченных этим лейблом (Б) число раз, когда предложение начиналось с этого лейбла (В) число раз, когда предложение заканчивалось этим лейблом\n",
    "\n",
    "$\\mathbf{Задание.}$ По аналогии допишите сниппет: запишите в переменную tag_ends словарь, для каждого лейбла указывающий, сколько позиций. $\\textit{Хинт.}$ Можете воспользоваться специальным словарём Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHaQEdIdHx3T",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#функция для подсчёта (А)\n",
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][hash_feature_list(word)] += 1\n",
    "    return d\n",
    "\n",
    "#считаем (Б)\n",
    "def starting_counts(sequences):\n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "starts_tag = [i[0] for i in data.Y]\n",
    "tag_starts = starting_counts(starts_tag)\n",
    "\n",
    "#считаем (В)\n",
    "def ending_counts(sequences):\n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "end_tag = [i[len(i) - 1] for i in data.Y]\n",
    "tag_ends = ending_counts(end_tag)\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "words = [word for i, (word, tag) in enumerate(data.training_set.stream())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdeSfSyfHx3X"
   },
   "source": [
    "С помощью библиотеки $\\href{https://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html}{pomegranate}$ создадим и инициируем правильными весами скрытую марковскую модель. \n",
    "\n",
    "$\\mathbf{Задание.}$ Посмотрите, как вычисляется вероятность результата (emission probabilities), матрица переходов (transition probabilities) и вектор начального распределения, и по аналогии вычислите конечное вероятностное распределение (т.е. вектор, в каждой компоненте которого содержится вероятность того, что предложение кончается соответствующим тегом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x136hzmfHx3e"
   },
   "outputs": [],
   "source": [
    "EPS = 1e-11\n",
    "\n",
    "#инициализация \n",
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "words = [word for i, (word, tag) in enumerate(data.stream())]\n",
    "\n",
    "#посчитаем (А)\n",
    "tags_count=unigram_counts(tags)\n",
    "tag_words_count=pair_counts(tags,words)\n",
    "\n",
    "starting_tag_list=[i[0] for i in data.Y]\n",
    "ending_tag_list=[i[-1] for i in data.Y]\n",
    "\n",
    "#количество ралзличных тегов, появлявшихся в начале и конце предложения\n",
    "starting_tag_count=starting_counts(starting_tag_list)\n",
    "ending_tag_count=ending_counts(ending_tag_list)      \n",
    "\n",
    "#список всех хотя бы раз задействованных состояний\n",
    "to_pass_states = []\n",
    "\n",
    "# вычислим вероятности результата:\n",
    "# переберём список тегов вместе со списком\n",
    "# фича-векторов, помеченных соответствующим лейблом\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    \n",
    "    #Сумма вероятностей наблюдений по всем\n",
    "    #векторам-состояниям, имевшим метку tag \n",
    "    total = float(sum(words_dict.values()))\n",
    "    \n",
    "    #для каждого вектора-состояния вычислим его условную вероятность\n",
    "    #при текущем состоянии как его долю в сумме вероятностей по всем векторам\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)\n",
    "\n",
    "basic_model.add_states()\n",
    "\n",
    "#вычислим начальное распределение вероятностей\n",
    "start_prob={}\n",
    "\n",
    "# Для каждого тега посчитаем его компонент в начальном распределении\n",
    "# как долю случаев, когда текущий тег был первым в предложении относительно\n",
    "# всех появлений данного тега в датасете\n",
    "for tag in tags:\n",
    "    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]\n",
    "\n",
    "#добавляем соответсвующие ребро в модель\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])\n",
    "\n",
    "#аналогично вычислим итоговое распределение\n",
    "end_prob={}\n",
    "for tag in tags:\n",
    "    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\n",
    "\n",
    "\n",
    "#добавляем соответсвующие ребро в модель\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])\n",
    "\n",
    "# вычислим матрицу переходов между скрытыми состояниями:\n",
    "# перебем все встреченные биграммы для каждой \n",
    "# пары лейблов key = (fr, to) посчитаем вероятность перехода \n",
    "# из fr в to, как частное количества би-грамм (fr, to) \n",
    "# и общего количества вхождений тега fr.\n",
    "transition_prob_pair={}\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]\n",
    "for tag_state in to_pass_states :\n",
    "    for next_tag_state in to_pass_states :\n",
    "        try:\n",
    "            prob_pair = transition_prob_pair[(tag_state.name,next_tag_state.name)]\n",
    "        except KeyError:\n",
    "            #если такой биграммы не встретилось -- возвращаем очень маленькое значение\n",
    "            prob_pair = EPS\n",
    "        basic_model.add_transition(tag_state,next_tag_state,prob_pair)\n",
    "\n",
    "#\"выпекаем\" HMM\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTFmqe0mHx3i"
   },
   "outputs": [],
   "source": [
    "# функция, вычисляющая наиболее вероятную последовательность\n",
    "# скрытых состояний (частей речи) с помощью алгоритма Витерби\n",
    "def simplify_decoding(X, model, extracted=False):\n",
    "    if(not extracted):\n",
    "        X = extract_features(X)\n",
    "    _, state_path = model.viterbi([hash_feature_list(t) for t in X])\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_W3FW3L_Hx3l"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sentence Key: b100-56005\n\n('But', ',', 'at', 'the', 'start', ',', 'his', 'new', 'life', 'felt', 'invigorating', '.'):\n-----------------\nPredicted labels:\n-----------------\n['CONJ', '.', 'ADP', 'DET', 'NOUN', '.', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.']\n\nActual labels:\n--------------\n('CONJ', '.', 'ADP', 'DET', 'NOUN', '.', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.')\n\n\nSentence Key: b100-15268\n\n('Then', ',', 'too', ',', 'European', 'drivers', 'have', 'reputations', 'for', 'being', 'somewhat', 'crazy', 'on', 'the', 'road', 'and', 'some', 'Americans', 'are', 'not', 'particularly', 'keen', 'on', 'getting', 'mixed', 'up', 'with', 'them', '.'):\n-----------------\nPredicted labels:\n-----------------\n['ADV', '.', 'ADV', '.', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'ADP', 'VERB', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'CONJ', 'DET', 'NOUN', 'VERB', 'ADV', 'ADV', 'VERB', 'PRON', 'VERB', 'VERB', 'PRT', 'ADP', 'NOUN', '.']\n\nActual labels:\n--------------\n('ADV', '.', 'ADV', '.', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'ADP', 'VERB', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'CONJ', 'DET', 'NOUN', 'VERB', 'ADV', 'ADV', 'ADJ', 'ADP', 'VERB', 'VERB', 'PRT', 'ADP', 'PRON', '.')\n\n\n"
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:2]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(str(data.sentences[key].words) + str(\":\\n-----------------\"))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи и оценка результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5TEZtiTHx3p"
   },
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"функция, предсказывающая лейбл для каждого слова каждого предложения из X\"\"\"\n",
    "    answer = []\n",
    "    for observations in X:\n",
    "        most_likely_tags = simplify_decoding(observations, model, extracted=True)\n",
    "        answer.append(tuple(most_likely_tags))\n",
    "    return tuple(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь хотелось бы как-то оценить результаты работы нашей модели.\n",
    "\n",
    "Самой простой и наивной score-функцией для задачи классификации является точность. Точность вычисляется как частное правильно классифицированных примеров и всех примеров. \n",
    "\n",
    "$\\bf{Задание.} Если вы всё сделали грамотно -- ваша модель получит точность более 0.7. Если нет -- попробуйте добавить признаков в кодирование слов.\n",
    "\n",
    "Однако такая метрика имеет ряд недостатков и зачастую является непоказательной. Пожалуй, наиболее популярной функцией оценки качества классифицирующей модели является $\\href{https://en.wikipedia.org/wiki/F1_score}{F1-score}$ (F1-мера). F1-мера считается отдельно для каждого класса $k$, и является средним гармоническим между $\\bf{precision}$ (доля правильно классифицированных примеров из $k$ среди всех примеров, классифицированных как k) и $\\bf{recall}$ (доля правильно классифицированных примеров из $k$ среди всех примеров из k). \n",
    "\n",
    "$\\bf{Задание.}$ Реализуйте функцию F1score. При желании можете воспользоваться готовым решением из библиотеки sklearn, но написание \"ручками\" всячески приветствуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcY6Lic9Hx3u",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Точность: 88.43%\nF1-scores: [0.99973378 0.89485428 0.97490569 0.86641814 0.98863636 0.9755603\n 0.9849016  0.80642805 0.92306396 0.87944757 0.9831758  0.61354582]\nmacro F1 score: 90.76%\n"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def accuracy(Y_pred, Y_true):\n",
    "    correct, total = 0, 0\n",
    "    for i in range(len(Y_pred)):\n",
    "        correct = correct + sum([int(f == s) for f, s in zip(Y_pred[i], Y_true[i])])\n",
    "        total += len(Y_pred[i])\n",
    "    return float(correct / total)\n",
    "\n",
    "def f1score(Y_pred, Y_true, macro = False):\n",
    "    \"\"\"macro:\n",
    "        если False: вернуть список, содержащий значения f1-score для каждого лейбла\n",
    "        если True: вернуть значение макро f1-score\n",
    "    \"\"\"\n",
    "    #TODO: YOUR CODE\n",
    "    if(macro):\n",
    "        return 0\n",
    "    else:\n",
    "        return [0 for _ in data.tagset]\n",
    "\n",
    "Y_pred = predict(data.testing_set.X, basic_model)\n",
    "\n",
    "acc = accuracy(Y_pred, data.testing_set.Y)\n",
    "print(\"Точность: {:.2f}%\".format(100 * acc))\n",
    "\n",
    "assert(acc < 0.7, \"Вы получили точность менее 0.7 на тестовом датасете. Попробоуйте изменить список признаков.\")\n",
    "\n",
    "f1 = f1_score(y_true=MultiLabelBinarizer().fit_transform(data.testing_set.Y), y_pred=MultiLabelBinarizer().fit_transform(Y_pred), average=None)\n",
    "print(\"F1-scores: \" + str(f1))\n",
    "\n",
    "mf1 = f1_score(y_true=MultiLabelBinarizer().fit_transform(data.testing_set.Y), y_pred=MultiLabelBinarizer().fit_transform(Y_pred), average='macro')\n",
    "print(\"macro F1 score: {:.2f}%\".format(100 * mf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit55244b91c83749b1bf07fc66448ae3c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}