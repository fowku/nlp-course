{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа #6: Introduction to neural nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предстоит познакомиться с искуственными нейронными сетями. Вообще нейронные сети применимы к широкому спектру задач, однако в области NLP они, как правило, решают задачи классификации. Поэтому сегодня мы будем обучать нейронные сети именно этой задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import load_wine, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим самый базовый датасет для классифицирующих моделей -- $\\href{https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html}{iris}$. Каждая запись в этом датасете предсталяет из себя набор численных характеристик некоторого цветка ириса, целевым параметром является лейбл класса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Число цветков (семлов) = 150\n Число численных характеристик (фичей) цветка = 4\n Количество классов = 3\n\n Вектора фичей выглядят так:\n\n [[5.1 3.5 1.4 0.2]\n [4.8 3.1 1.6 0.2]\n [5.  2.  3.5 1. ]\n [5.5 2.6 4.4 1.2]\n [6.9 3.2 5.7 2.3]]\n А целевые значения, лейблы классов, выглядят так:\n [0 0 1 1 2]\n"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "n_categories = max(y) + 1\n",
    "print(\" Число цветков (семлов) = {}\\n Число численных характеристик (фичей) цветка = {}\\n Количество классов = {}\\n\".format(X.shape[0], X.shape[1], n_categories))\n",
    "print(\" Вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача классификации состоит в предсказании лейблов класса по данным векторам фичей. Эту задачу можно решать классифицирующими нейронными сетями. Чем мы и займёмся!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем мы приступим к написанию нейронной сети, давайте подготовим данные.\n",
    "\n",
    "Во-первых, хорошей практикой является нормализация данных. Нейронные сети лучше обучаются, если все численные характеристики в векторах численных характеристик располагаются в отрезке [0, 1]. Это можно сделать с помощью MinMaxScaler, который для каждой фичи находит её максимальное и минимальное значение среди всех семплов и относительно них нормализует соответствующий компонент каждого вектора фич.\n",
    "\n",
    "Во-вторых, категориальные характеристики. Целевым значением является лейбл класса, который может принимать только три значения: {0, 1, 2}. Если мы будем хранить это значение как число, то предопредилим нежелательные отношения между классами -- так, класс $1$ будет рассматриваться моделью как более близкий к классу $2$, чем класс $0$, что вовсе не детерминированно условиями задачи или входными данными. Нужно получить такое представление классов, чтоб расстояние между предсталениями двух любых различных классов было одинаково. Это можно сделать с помощью техники $\\href{https://en.wikipedia.org/wiki/One-hot}{One-Hot Encoding}$: давайте кодировать категориальные характеристики с помощью векторов размерности $n = $число различных классов. Каждый такой вектор будет содержать нули во всех своих компонентах кроме той, которая соответствует номеру класса, который мы пытаемся кодировать: в этой компоненте будет стоять единица. Например, последовательность [1, 0, 2, 0...] будет закодирована как [[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Теперь вектора фичей выглядят так:\n\n [[0.22222222 0.625      0.06779661 0.04166667]\n [0.13888889 0.45833333 0.10169492 0.04166667]\n [0.19444444 0.         0.42372881 0.375     ]\n [0.33333333 0.25       0.57627119 0.45833333]\n [0.72222222 0.5        0.79661017 0.91666667]]\n А целевые значения, лейблы классов, выглядят так:\n [[1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X, y)\n",
    "y = np.array([np.array([0.0 if i != cat else 1.0 for i in range(n_categories)]) for cat in y])\n",
    "\n",
    "print(\" Теперь вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разделим данные на тренировочные и тестовые. Для нейронных сетей мы будем использовать библиотеку $\\href{https://pytorch.org/}{PyTorch}$, обладающую своими типами и строгой типизацией, так что нам нужно привести данные к её типам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train))\n",
    "X_test = Variable(torch.from_numpy(X_test))\n",
    "y_train = Variable(torch.from_numpy(y_train))\n",
    "y_test = Variable(torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте придумаем архитектуру классифицирующей нейронной сети.\n",
    "\n",
    "Число нейронов во входном слое должно соответствовать размерности фича-веткора, в нашем случае это 4. Число нейронов в выходном слое должно соответствовать числу классов, в нашем случае это 3.\n",
    "\n",
    "Очень часто в задачах классификации в качестве функции активации для последнего слоя нейронов используется функция $\\href{https://ru.wikipedia.org/wiki/Softmax}{SoftMax}$. Эта функция преобразовывает вектор таким образом, что каждая его компонента становится неотрицательной, а сумма компонент равняется 1. Из-за этого компоненту под номером i получающегося вектора можно рассматривать как степень уверенности нашей сети в том, что обрабатываемый семпл принадлежит к классу i. Именно таким образом (one-hot) закодированы правильные ответы.\n",
    "\n",
    "Опишем нейронную сеть с одним скрытым слоем при помощи библиотеки $\\href{https://pytorch.org/}{PyTorch}$. Это актуальный и очень серёьзный, но при этом гибкий и удобный инструмент. Смотрите, как просто и интуитивно-понятно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_size = X.shape[1]     #число нейронов входного слоя\n",
    "h_size = 10             #число нейронов скрытого слоя\n",
    "o_size = n_categories   #число нейронов выходного слоя\n",
    "\n",
    "intuitive_model = nn.Sequential(\n",
    "    nn.Linear(i_size, h_size), #линейная операция сложения, числа соответствуют размерностям входного и выходного векторов\n",
    "    nn.ReLU(),                 #функция активации RELU\n",
    "    nn.Linear(h_size, o_size),\n",
    "    nn.Softmax()               #функция активации SoftMax\n",
    ").double()                     #строгая типизация строга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь опишем функцию train_batch, которая принимает пачку примеров, вычисляет усреднённую ошибку нейронной сети на них и соответствующим образом меняет её веса. Это возможно благодаря методу обратного распространения ошибки (gradient descent), который, конечно, помните из лекции. В ходе обратного распространения ошибки необходимо считать градиенты от функции ошибки по векторам весов сети.\n",
    "\n",
    "Хорошие новости: нам не придётся вручную считать градиенты, потому что torch может делать это сам. Нам нужно лишь определить оптимайзер (возьмем стохастический градиентный спуск), функцию ошибки (возьмем бинарную кросс-энтропию) и скорость обучения (learning rate, возьмем 0.05).\n",
    "\n",
    "Когда эти параметры определены, можно вычислять градиенты и менять веса при помощи простых команд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss() #функция ошибки\n",
    "\n",
    "opt= torch.optim.SGD(intuitive_model.parameters(),lr=0.05) #оптимайзер и скорость обучения\n",
    "\n",
    "def train_batch(model, X, y):  # функция, обучающая модель на пачке примеров\n",
    "    y_pred = model(X)          # получим ответ сети для каждого из примеров\n",
    "    ls = loss(y_pred, y)       # вычислим функцию ошибки\n",
    "    opt.zero_grad()            \n",
    "    ls.backward()              # посчитаем градиенты\n",
    "    opt.step()                 # изменяем веса сети\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию train_model, обучающую нейронную сеть в течение n_epochs эпох. Одна эпоха соответствует одному полному просмотру тренировочного датасета. Как уже было отмечено, мы будем обучать сеть пачками (batches) примеров. В рамках одной эпохи мы будем перемешивать тренировочную выборку, делить её на пачки по batch_size примеров, и \"скармливать\" их модели.\n",
    "\n",
    "Давайте обучим нашу сеть на 20 эпохах и оценим результаты её работы на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch #2\n\nError on train = 0.6367194898963375\n\nError on test = 0.6260974186140281\n\nEpoch #4\n\nError on train = 0.6090810150708529\n\nError on test = 0.6036118343894187\n\nEpoch #6\n\nError on train = 0.5836718782216478\n\nError on test = 0.5761610504931518\n\nEpoch #8\n\nError on train = 0.5537652206874978\n\nError on test = 0.5408075602668415\n\nEpoch #10\n\nError on train = 0.5199259521889242\n\nError on test = 0.5015023460377549\n\nEpoch #12\n\nError on train = 0.485580348634249\n\nError on test = 0.46290809672437233\n\nEpoch #14\n\nError on train = 0.4501996711308475\n\nError on test = 0.42720558410121895\n\nEpoch #16\n\nError on train = 0.42157699004169147\n\nError on test = 0.39178336244463235\n\nEpoch #18\n\nError on train = 0.3913348130462743\n\nError on test = 0.3570772815670386\n\nEpoch #20\n\nError on train = 0.36406527405166506\n\nError on test = 0.33430630390760024\n\n"
    }
   ],
   "source": [
    "batch_size = 4 #рамзер пачки\n",
    "\n",
    "def train_model(model, n_epochs, print_every):\n",
    "    last_test_loss = None\n",
    "    for epoch in range(n_epochs):\n",
    "        n = X_train.shape[0]\n",
    "        random_permutation = torch.randperm(n) #возьмем случайную перестановку примеров\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(0,n,batch_size):\n",
    "            indices = random_permutation[i:i+batch_size] #индексы примеров очередной пачки\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices] # \"скормим\" их модели\n",
    "            total_loss += train_batch(model, batch_x, batch_y) #посчитаем суммарную ошибку на трейне\n",
    "                \n",
    "        if(print_every > 0 and (epoch + 1) % print_every == 0):\n",
    "            pred = intuitive_model(X_test)               #посчитает результат и ошибку на тесте\n",
    "            last_test_loss = loss(pred, y_test)\n",
    "            print(\"Epoch #{}\\n\".format(epoch + 1))\n",
    "            print(\"Error on train = {}\\n\".format(total_loss / (n // batch_size)))\n",
    "            print(\"Error on test = {}\\n\".format(loss(pred, y_test)))\n",
    "            \n",
    "    return last_test_loss\n",
    "    \n",
    "ll = train_model(intuitive_model, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте оценим нашу модель на тестовом сете -- выведем confusion matrix и число неверно классифицированных семплов.\n",
    "\n",
    "Confusion matrix -- матрица размерности n_classes x n_classes, число в i-й строке и j-м столбце соответствует количеству примеров из класса i, которые были классифицированы как j. Сумма главной диагонали, таким образом, соответствует количеству правильных ответов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[20  0  0]\n [ 0  7  6]\n [ 0  1 16]]\nЧисло неправильно классифицированных примеров = 7\n"
    }
   ],
   "source": [
    "def evaluate_model(model, print_matrix = True):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test) #делаем предсказание на тесте\n",
    "        y_pred_new = torch.argmax(y_pred, dim=1) #ответ сети -- номер компоненты вектора с наибольшим значением. Этот номер соответстует классу.\n",
    "        y_test_true = torch.argmax(y_test, dim=1) #обратно кодируем правильные ответы, чтоб узнать правильные лейблы.\n",
    "        if(print_matrix):\n",
    "            print(confusion_matrix(y_test_true, y_pred_new))\n",
    "        missclass = sum([int(pred != true) for (pred, true) in zip(y_pred_new, y_test_true)]) #вот так можно посчитать число неправильных ответов\n",
    "    return missclass\n",
    "\n",
    "print(\"Число неправильно классифицированных примеров = {}\".format(evaluate_model(intuitive_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, недурно! Но, может быть, можно лучше? Может быть можно выбрать такие параметры сети, что ни один пример не будет классифицирован неправильно?\n",
    "\n",
    "Есть ещё один способ задания сети с помощью PyTorch: кастомизация нейросети путём наследования от базового класса torch.nn.module. Наследующий класс должен реализовать функцию-констукртор __init__, инициализирущую веса сети, и функцию forward, совершающую прямое прохождение по сети. В такой имплементации любые параметры сети (например, количество нейронов в скрытом слое или функция активации на нём) можно сделать параметрами её конструктора.\n",
    "\n",
    "Давайте реализуем класс CustomModel, соответствующий нейронной сети с одним скрытым слоем, число нейронов и функция активации которого будет передаваться на стадии инициализации. Функции train_batch и train_model можно сделать методами нового класса, а все вспомогательные объекты (оптимизатор, learning rate, batch_size) -- его параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=8, activation=torch.nn.ReLU(), batch_size=4, lr=0.05): #конструктор сети принимает на вход число нейронов скрытого слоя и активирующую его функцию\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.l1 = torch.nn.Linear(i_size, hidden_size)   \n",
    "        self.act = activation                              #конкретная функция activation будет получена как параметр\n",
    "        self.l2 = torch.nn.Linear(hidden_size, o_size)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=0.05)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \"\"\"NB: Во всём остальном -- то же самое, что и intuitive_model\"\"\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h=self.act(self.l1(x))\n",
    "        return self.softmax(self.l2(h))\n",
    "\n",
    "    def train_batch(self, X, y):\n",
    "        y_pred = self(X)\n",
    "        ls = loss(y_pred, y)\n",
    "        self.opt.zero_grad()\n",
    "        ls.backward()\n",
    "        self.opt.step()\n",
    "        return ls\n",
    "    \n",
    "    def train_model(self, n_epochs, print_every):\n",
    "        last_test_loss = None\n",
    "        for epoch in range(n_epochs):\n",
    "            n = X_train.shape[0]\n",
    "            random_permutation = torch.randperm(n)\n",
    "            total_loss = 0\n",
    "            for i in range(0,n,self.batch_size):\n",
    "                indices = random_permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "                total_loss += self.train_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте найдем лучшие значения hidden_size и activation, а также оптимальное число обучающих эпох для нашей задачи. Есть множество техник поиска гиперпараметров, самый простой из них -- выбрать несколько значений для разных параметров, перебрать все комбинации и найти лучшее сочетание. Давайте это и сделаем с помощью функции evaluate_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best paramset: n_epochs=25, activation_function =Tanh(), hidden_size = 3\n. Micclassifications = 0.\n\n"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "activations = [torch.nn.ReLU(), torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.SELU(), torch.nn.Hardtanh()] #перебираемые функции активации\n",
    "hidden_sizes = [3, 4, 7, 10, 20] #перебираемое число нейронов скрытого слоя\n",
    "epochs = [25, 50] #перебираемое число эпох\n",
    "best_misses = len(y_test) #лучший результат в смысле количества неправильно классифицированных семплов\n",
    "best_paramset = None\n",
    "\n",
    "for paramset in (itertools.product(epochs, activations, hidden_sizes)): #paramset -- tuple, содержащий значения каждого из параметров\n",
    "    e, a, h = paramset\n",
    "    model = CustomModel(hidden_size = h, activation = a).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "    model.train_model(e, 0) #тренируем её в течение %e% эпох \n",
    "    misses = evaluate_model(model, print_matrix = False) #количеств неправильно классифицируемых примеров\n",
    "    if(best_misses > misses): #если результат лучше -- обновляем\n",
    "        best_misses = misses\n",
    "        best_paramset = paramset\n",
    "        \n",
    "print(\"Best paramset: n_epochs={}, activation_function ={}, hidden_size = {}\\n. Micclassifications = {}.\\n\".format(best_paramset[0],best_paramset[1],best_paramset[2],best_misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: а теперь ваша очередь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше домашнее задание состоит в том, чтоб по аналогии построить классифицирующую модель для датасета wine. Вам необходимо построить модель, предсказывающую класс вина по его численным характеристикам. Если точнее, от вас требуется:\n",
    "\n",
    "1. Подготовить данные: произвести нормализацию численных данных и one-hot векторизацию категориальных, разделить выборку на тренировочную и тестовую\n",
    "2. Построить и обучить классифицирующую модель на основе нейронной сети, содержащую один скрытый слой. Возьмите такое количество эпох обучения, которое достататочно для того, чтоб функция ошибки перестала падать. Все остальные параметры сети -- на ваше усмотрение.\n",
    "3. Выберите несколько значений параметров hidden_size (число нейронов скрытого слоя), batch_size (число примеров в пачке) и learning_rate(скорость обучения). Опираясь на функцию evaluate_model, найдите лучшую комбинацию параметров. Выведите confusion matrix на тесте для нейронной сети с оптимальными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Число проб вина (семлов) = 178\n Число численных характеристик (фичей) каждой пробы вина = 13\n Количество классов = 3\n\n Вектора фичей выглядят так:\n\n [[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n [1.373e+01 1.500e+00 2.700e+00 2.250e+01 1.010e+02 3.000e+00 3.250e+00\n  2.900e-01 2.380e+00 5.700e+00 1.190e+00 2.710e+00 1.285e+03]\n [1.233e+01 1.100e+00 2.280e+00 1.600e+01 1.010e+02 2.050e+00 1.090e+00\n  6.300e-01 4.100e-01 3.270e+00 1.250e+00 1.670e+00 6.800e+02]\n [1.208e+01 1.830e+00 2.320e+00 1.850e+01 8.100e+01 1.600e+00 1.500e+00\n  5.200e-01 1.640e+00 2.400e+00 1.080e+00 2.270e+00 4.800e+02]\n [1.145e+01 2.400e+00 2.420e+00 2.000e+01 9.600e+01 2.900e+00 2.790e+00\n  3.200e-01 1.830e+00 3.250e+00 8.000e-01 3.390e+00 6.250e+02]\n [1.350e+01 3.120e+00 2.620e+00 2.400e+01 1.230e+02 1.400e+00 1.570e+00\n  2.200e-01 1.250e+00 8.600e+00 5.900e-01 1.300e+00 5.000e+02]]\n А целевые значения, лейблы классов, выглядят так:\n [0 0 1 1 1 2]\n"
    }
   ],
   "source": [
    "X, y = load_wine(return_X_y=True)\n",
    "n_categories = max(y) + 1\n",
    "print(\" Число проб вина (семлов) = {}\\n Число численных характеристик (фичей) каждой пробы вина = {}\\n Количество классов = {}\\n\".format(X.shape[0], X.shape[1], n_categories))\n",
    "print(\" Вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем фичи при помощи MinMaxScaler и перекодируем лейблы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Теперь вектора фичей выглядят так:\n\n [[0.84210526 0.1916996  0.57219251 0.25773196 0.61956522 0.62758621\n  0.57383966 0.28301887 0.59305994 0.37201365 0.45528455 0.97069597\n  0.56134094]\n [0.71052632 0.15019763 0.71657754 0.61340206 0.33695652 0.69655172\n  0.61392405 0.30188679 0.6214511  0.37713311 0.57723577 0.52747253\n  0.71825963]\n [0.34210526 0.07114625 0.49197861 0.27835052 0.33695652 0.36896552\n  0.15822785 0.94339623 0.         0.16979522 0.62601626 0.14652015\n  0.28673324]\n [0.27631579 0.21541502 0.51336898 0.40721649 0.11956522 0.2137931\n  0.24472574 0.73584906 0.38801262 0.09556314 0.48780488 0.36630037\n  0.14407989]\n [0.11052632 0.32806324 0.56684492 0.48453608 0.2826087  0.66206897\n  0.51687764 0.35849057 0.44794953 0.16808874 0.2601626  0.77655678\n  0.24750357]\n [0.65       0.47035573 0.67379679 0.69072165 0.57608696 0.14482759\n  0.25949367 0.16981132 0.26498423 0.62457338 0.08943089 0.01098901\n  0.15834522]]\n А целевые значения, лейблы классов, выглядят так:\n [[1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X, y)\n",
    "y = np.array([np.array([0.0 if i != cat else 1.0 for i in range(n_categories)]) for cat in y])\n",
    "\n",
    "print(\" Теперь вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "поделим на тестовые и тренировочные сеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train))\n",
    "X_test = Variable(torch.from_numpy(X_test))\n",
    "y_train = Variable(torch.from_numpy(y_train))\n",
    "y_test = Variable(torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_size = X.shape[1]     #число нейронов входного слоя\n",
    "h_size = 10             #число нейронов скрытого слоя\n",
    "o_size = n_categories   #число нейронов выходного слоя\n",
    "\n",
    "intuitive_model = nn.Sequential(\n",
    "    nn.Linear(i_size, h_size), #линейная операция сложения, числа соответствуют размерностям входного и выходного векторов\n",
    "    nn.ReLU(),                 #функция активации RELU\n",
    "    nn.Linear(h_size, o_size),\n",
    "    nn.Softmax()               #функция активации SoftMax\n",
    ").double()                     #строгая типизация строга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обучим на 20 эпохах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[15  0  0]\n [ 0 25  1]\n [ 0  0 18]]\nЧисло неправильно классифицированных примеров = 1\n"
    }
   ],
   "source": [
    "opt = torch.optim.SGD(intuitive_model.parameters(),lr=0.05) #оптимайзер и скорость обучения\n",
    "batch_size = 4 #рамзер пачки\n",
    "ll = train_model(intuitive_model, 20, 21)\n",
    "print(\"Число неправильно классифицированных примеров = {}\".format(evaluate_model(intuitive_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле все итак норм, но дальше по примеру сделаю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порандомим с количеством эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best paramset: n_epochs=15, activation_function =Sigmoid(), hidden_size = 3\n. Micclassifications = 0.\n\n"
    }
   ],
   "source": [
    "activations = [torch.nn.ReLU(), torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.SELU(), torch.nn.Hardtanh()] #перебираемые функции активации\n",
    "hidden_sizes = [3, 4, 7, 10, 20] #перебираемое число нейронов скрытого слоя\n",
    "epochs = [1, 5, 10, 15, 20, 30, 40] #перебираемое число эпох\n",
    "best_misses = len(y_test) #лучший результат в смысле количества неправильно классифицированных семплов\n",
    "best_paramset = None\n",
    "\n",
    "for paramset in (itertools.product(epochs, activations, hidden_sizes)): #paramset -- tuple, содержащий значения каждого из параметров\n",
    "    e, a, h = paramset\n",
    "    model = CustomModel(hidden_size = h, activation = a).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "    model.train_model(e, 0) #тренируем её в течение %e% эпох \n",
    "    misses = evaluate_model(model, print_matrix = False) #количеств неправильно классифицируемых примеров\n",
    "    if(best_misses > misses): #если результат лучше -- обновляем\n",
    "        best_misses = misses\n",
    "        best_paramset = paramset\n",
    "        \n",
    "print(\"Best paramset: n_epochs={}, activation_function ={}, hidden_size = {}\\n. Micclassifications = {}.\\n\".format(best_paramset[0],best_paramset[1],best_paramset[2],best_misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем перебирать число нейронов, примеров в пачке и скорость обучения, в качестве функции активации будем использовать ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best paramset: batch = 3, lr =0.05, hidden_size = 4\n. Micclassifications = 0.\n\n"
    }
   ],
   "source": [
    "hidden_sizes = [3, 4, 7, 10, 20] #перебираемое число нейронов скрытого слоя\n",
    "batch_sizes = [1, 3, 5, 7, 10] # количество примеров в пачке\n",
    "lrs = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1] # скорость обучения\n",
    "best_misses = len(y_test) #лучший результат в смысле количества неправильно классифицированных семплов\n",
    "best_paramset = None\n",
    "\n",
    "for paramset in (itertools.product(batch_sizes, lrs, hidden_sizes)): #paramset -- tuple, содержащий значения каждого из параметров\n",
    "    b, l, h = paramset\n",
    "    model = CustomModel(batch_size=b, lr=l, hidden_size=h).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "    model.train_model(15, 0) #тренируем её в течение 15 эпох \n",
    "    misses = evaluate_model(model, print_matrix = False) #количеств неправильно классифицируемых примеров\n",
    "    if(best_misses > misses): #если результат лучше -- обновляем\n",
    "        best_misses = misses\n",
    "        best_paramset = paramset\n",
    "        \n",
    "print(\"Best paramset: batch = {}, lr ={}, hidden_size = {}\\n. Micclassifications = {}.\\n\".format(best_paramset[0],best_paramset[1],best_paramset[2],best_misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[15  0  0]\n [ 0 25  1]\n [ 0  0 18]]\n"
    }
   ],
   "source": [
    "model = CustomModel(batch_size=3, lr=0.05, hidden_size=4).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "model.train_model(15, 0) #тренируем её в течение 15 эпох \n",
    "misses = evaluate_model(model, print_matrix = True) #количеств неправильно классифицируемых примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit55244b91c83749b1bf07fc66448ae3c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}